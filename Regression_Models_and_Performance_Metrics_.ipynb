{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Supervised Learning: Regression Models and Performance Metrics |"
      ],
      "metadata": {
        "id": "CIzoEzI8daMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 1.What is Simple Linear Regression (SLR)? Explain its purpose.\n",
        "\n",
        "Ans--Simple linear regression is a statistical method used to model the relationship between two variables: one independent variable (predictor) and one dependent variable (response). Its primary purpose is to predict outcomes and understand relationships between these variables.\n",
        "\n",
        "This technique works by fitting a straight line, known as the regression line, to the data points in a way that minimizes the sum of squared differences (residuals) between the observed values and the predicted values. The equation for this line is typically expressed as:\n",
        "\n",
        "-     ŷ = b0 + b1x\n",
        "\n",
        "Here, b0 is the y-intercept, b1 is the slope, and x is the independent variable."
      ],
      "metadata": {
        "id": "c4wNWdiqdlgT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "Ans--Linear regression relies on several key assumptions to ensure the validity and reliability of its results. These assumptions are critical for making accurate predictions and valid statistical inferences. Below are the primary assumptions:\n",
        "\n",
        "-  **Linearity:** The relationship between the independent and dependent variables must be linear. This ensures that changes in the independent variable result in proportional changes in the dependent variable.\n",
        "\n",
        "-  **Homoscedasticity:** The residuals (differences between observed and predicted values) should have constant variance across all levels of the independent variables. If the variance changes (heteroscedasticity), it can lead to inefficient estimates and unreliable hypothesis tests.\n",
        "\n",
        "-   **Normality of Residuals:** The residuals should follow a normal distribution. This assumption is crucial for valid hypothesis testing and confidence intervals.\n",
        "\n",
        "-   **Independence of Errors:** The residuals should not be correlated with one another. This is particularly important in time-series data, where autocorrelation can occur if errors at one time point influence errors at another.\n",
        "\n",
        "-   **Lack of Multicollinearity:** The independent variables should not be highly correlated with each other. Multicollinearity can inflate standard errors, making it difficult to assess the individual impact of predictors.\n",
        "\n",
        "-   **Absence of Endogeneity:** The independent variables should not be correlated with the error term. Endogeneity leads to biased and inconsistent coefficient estimates, undermining the model's validity."
      ],
      "metadata": {
        "id": "XAi69TSGd_0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 3.Write the mathematical equation for a simple linear regression model and\n",
        "explain each term.\n",
        "\n",
        "Ans--The general equation for a simple linear regression model is:\n",
        "\n",
        "-     y = a + bx\n",
        "\n",
        "Where:\n",
        "\n",
        "-   y: The dependent variable (response variable) whose value we aim to predict.\n",
        "\n",
        "-   x: The independent variable (predictor variable) that influences the dependent variable.\n",
        "\n",
        "-   a: The intercept, representing the value of y when x = 0. It is the point where the regression line crosses the y-axis.\n",
        "\n",
        "-   b: The slope of the regression line, indicating the rate of change in y for a one-unit increase in x.\n",
        "\n",
        "**Explanation of Terms**\n",
        "\n",
        "-  **Dependent Variable (y):** This is the outcome or target variable that the model predicts. For example, in predicting house prices, y would represent the price of the house.\n",
        "\n",
        "-   **Independent Variable (x):** This is the input or explanatory variable that influences the dependent variable. For instance, in the house price example, x could represent the size of the house.\n",
        "\n",
        "-   **Intercept (a):** This is the starting value of y when x is zero. It provides a baseline prediction when no influence from the independent variable is present. However, in some contexts, the intercept may not always have a meaningful interpretation (e.g., predicting tree height when x = 0).\n",
        "\n",
        "-   **Slope (b):** This measures the strength and direction of the relationship between x and y. A positive slope indicates that as x increases, y also increases, while a negative slope suggests the opposite.\n",
        "\n",
        "**Key Insights**\n",
        "\n",
        "The equation assumes a linear relationship between the variables, meaning changes in the independent variable are associated with proportional changes in the dependent variable. The slope and intercept are calculated using statistical methods like the least squares approach, which minimizes the sum of squared differences between observed and predicted values.\n",
        "\n",
        "This model is widely used in predictive analytics, trend analysis, and understanding relationships between variables in fields like economics, biology, and machine learning."
      ],
      "metadata": {
        "id": "y8VkeCOYetra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 4.Provide a real-world example where simple linear regression can be\n",
        "applied.\n",
        "\n",
        "Ans--Advertising Spending and Sales Revenue\n",
        "\n",
        "**In this scenario, a company wants to understand how its advertising budget impacts its sales revenue. The company collects data over several months, recording the amount spent on advertising and the corresponding sales revenue generated during that period.**\n",
        "-  Independent Variable (X): Advertising Spending (in dollars)\n",
        "-  Dependent Variable (Y): Sales Revenue (in dollars)\n",
        "\n",
        "**Data Collection**\n",
        "\n",
        "-   The company gathers the following data over a few months:\n",
        "\n",
        "| Month | Advertising Spending (X) | Sales Revenue (Y) |\n",
        "|-------|---------------------------|-------------------|\n",
        "| 1     | $1,000                   | $10,000           |\n",
        "| 2     | $1,500                   | $15,000           |\n",
        "| 3     | $2,000                   | $20,000           |\n",
        "| 4     | $2,500                   | $25,000           |\n",
        "| 5     | $3,000                   | $30,000           |\n",
        "\n",
        "-   Applying Simple Linear Regression\n",
        "Using simple linear regression, the company can fit a line to this data to model the relationship between advertising spending and sales revenue. The regression equation might look like this:\n",
        "\n",
        "Y=β<sub>0</sub>\n",
        "​\n",
        " +β<sub>1</sub>\n",
        "​\n",
        " X\n",
        "\n",
        "Where:\n",
        "\n",
        "-   β<sub>0</sub>: is the y-intercept (the expected sales revenue when advertising spending is zero).\n",
        "-   β<sub>1</sub>:is the slope of the line (the expected change in sales revenue for each additional dollar spent on advertising).\n",
        "  \n",
        "**Interpretation of Results**\n",
        "After performing the regression analysis, the company finds that the regression equation is:\n",
        "\n",
        "Y=5,000+10X\n",
        "\n",
        "This means:\n",
        "\n",
        "-  For every additional dollar spent on advertising, the sales revenue increases by $10.\n",
        "\n",
        "-   If the company spends $0 on advertising, it can expect to generate $5,000 in sales revenue (the intercept).\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "By using simple linear regression, the company can make informed decisions about its advertising budget. If the analysis shows a strong positive relationship, the company may decide to increase its advertising spending to boost sales revenue further. This example illustrates how simple linear regression can be a powerful tool for businesses to understand and predict the impact of their investments on revenue."
      ],
      "metadata": {
        "id": "-5UqRd7XfiOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 5.What is the method of least squares in linear regression?\n",
        "\n",
        "Ans--When the value of the independent variable (x) is known, the Least-Squares Regression Line can be used to predict the value of the dependent variable (y). This method finds the line of best fit by minimizing the sum of squared differences between observed and predicted values.\n",
        "\n",
        "The regression line is expressed as:\n",
        "\n",
        "\n",
        "[ y = mx + c ]\n",
        "\n",
        "Where:\n",
        "\n",
        "\n",
        "-   m = slope of the line\n",
        "\n",
        "\n",
        "-   c = y-intercept\n",
        "\n",
        "Formulas: [ m = \\frac{n\\sum xy - (\\sum x)(\\sum y)}{n\\sum x^2 - (\\sum x)^2} ] [ c = \\frac{\\sum y - m(\\sum x)}{n} ]\n",
        "\n",
        "Once m and c are known, you can substitute any given x to predict y."
      ],
      "metadata": {
        "id": "QZnRIWmHp7nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Given data points\n",
        "x_values = [1, 2, 4, 6, 8]\n",
        "y_values = [3, 4, 8, 10, 15]\n",
        "n = len(x_values)\n",
        "sum_x = sum(x_values)\n",
        "sum_y = sum(y_values)\n",
        "sum_xy = sum(x*y for x, y in zip(x_values, y_values))\n",
        "sum_x2 = sum(x**2 for x in x_values)\n",
        "# Calculate slope (m) and intercept (c)\n",
        "m = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x**2)\n",
        "c = (sum_y - m * sum_x) / n\n",
        "# Predict y for a given x\n",
        "x_given = 5\n",
        "y_pred = m * x_given + c\n",
        "print(f\"Equation of line: y = {m:.2f}x + {c:.2f}\")\n",
        "print(f\"Predicted y for x={x_given}: {y_pred:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQAizkvuqa-x",
        "outputId": "0b022ad9-cc16-4dda-c2b1-14fca6207a06"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Equation of line: y = 1.68x + 0.96\n",
            "Predicted y for x=5: 9.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Points:**\n",
        "\n",
        "-  This method works best when data is linear and free from extreme outliers.\n",
        "\n",
        "-   It is widely used in predictive modeling, such as forecasting sales, estimating trends, or predicting physical measurements.\n",
        "\n",
        "-   For multiple variables, the concept extends to multiple linear regression.\n",
        "\n",
        "By applying the regression equation, any known x can be used to estimate the corresponding y with reasonable accuracy."
      ],
      "metadata": {
        "id": "9R7Q4U0gqdCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 6.What is Logistic Regression? How does it differ from Linear Regression?\n",
        "\n",
        "Ans--Linear Regression and Logistic Regression are both supervised learning algorithms, but they serve different purposes and produce different types of outputs.\n",
        "\n",
        "Linear Regression is used for predicting continuous numeric values. It models the relationship between input features and the target variable as a straight line, using the equation:\n",
        "\n",
        "\n",
        "-     Y = a + bX\n",
        "\n",
        "The output can be any real number, positive or negative, without restriction.\n",
        "\n",
        "Logistic Regression, on the other hand, is used for classification tasks, typically binary (0 or 1). It applies a sigmoid (logistic) function to the linear combination of inputs, which squashes the output to a range between 0 and 1, representing probabilities:\n",
        "\n",
        "-     P(Y=1) = 1 / (1 + e^-(a + bX))\n",
        "this probability can then be thresholded (e.g., at 0.5) to assign a class label.\n",
        "\n",
        "Correct Answer to the Question: Only logistic regressions have outputs between 0 and 1.\n",
        "\n",
        "Why this is correct:\n",
        "\n",
        "-  Linear Regression outputs are unbounded and can take any real value.\n",
        "\n",
        "-  Logistic Regression outputs are bounded between 0 and 1 due to the sigmoid transformation, making them interpretable as probabilities for classification tasks.\n"
      ],
      "metadata": {
        "id": "KVrMw1hWqrTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "# Sample data\n",
        "X = np.array([[1], [2], [3], [4], [5]])\n",
        "y_linear = np.array([2, 4, 6, 8, 10]) # Continuous target\n",
        "y_logistic = np.array([0, 0, 0, 1, 1]) # Binary target\n",
        "# Linear Regression\n",
        "lin_reg = LinearRegression().fit(X, y_linear)\n",
        "print(\"Linear Regression Prediction for 6:\", lin_reg.predict([[6]]))\n",
        "# Logistic Regression\n",
        "log_reg = LogisticRegression().fit(X, y_logistic)\n",
        "print(\"Logistic Regression Probability for 6:\", log_reg.predict_proba([[6]])[0,1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoKFLuHzreP_",
        "outputId": "58a6fe00-e7ae-4117-caaf-437741950add"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression Prediction for 6: [12.]\n",
            "Logistic Regression Probability for 6: 0.9264970551893357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the linear regression output is unbounded (12), while the logistic regression output is a probability (0.88) between 0 and 1.\n",
        "\n",
        "Key takeaway: If your task involves predicting probabilities or classifying outcomes, logistic regression is the right choice because its outputs are naturally constrained between 0 and 1. For continuous value prediction, linear regression is appropriate."
      ],
      "metadata": {
        "id": "b0Ay-qDorj4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 7.Name and briefly describe three common evaluation metrics for regression\n",
        "models.\n",
        "\n",
        "Ans--\n",
        "-   **Mean Absolute Error (MAE):** Measures the average magnitude of errors in a set of predictions, without considering their direction. It is the average over the test sample of the absolute differences between prediction and actual observation.\n",
        "\n",
        "-   **Mean Squared Error (MSE):** Measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value.\n",
        "\n",
        "-   **Root Mean Squared Error (RMSE):** The square root of the mean of the squared errors. It gives a relatively high weight to large errors.\n",
        "\n",
        "\n",
        "These metrics help assess the performance of regression models and guide improvements.\n"
      ],
      "metadata": {
        "id": "6HTIbHpXrlQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 8.What is the purpose of the R-squared metric in regression analysis?\n",
        "\n",
        "Ans--**Understanding R-squared**\n",
        "\n",
        "-   Definition: R-squared, also known as the coefficient of determination, quantifies how well the independent variables explain the variability of the dependent variable. It ranges from 0 to 1, where:\n",
        "\n",
        "-   0 indicates that the model does not explain any variability in the dependent variable.\n",
        "-   1 indicates that the model explains all the variability in the dependent variable.\n",
        "\n",
        "**Purpose and Applications**\n",
        "\n",
        "-   **Goodness of Fit:** R-squared serves as a measure of how well the regression model fits the data. A higher R-squared value suggests a better fit, meaning the model's predictions are closer to the actual data points.\n",
        "\n",
        "\n",
        "-   **Model Comparison:** It allows analysts to compare different regression models. By evaluating R-squared values, one can determine which model better explains the observed data variations.\n",
        "\n",
        "\n",
        "-   **Interpretation of Variance:** R-squared helps in understanding the proportion of the total variance in the dependent variable that is accounted for by the independent variables. This insight is crucial for assessing the effectiveness of the model.\n",
        "\n",
        "\n",
        "-   **Communication Tool:** It provides a straightforward way to communicate the predictive power of a model to stakeholders who may not have a technical background, making it easier to convey the model's effectiveness.\n",
        "\n",
        "**Limitations**\n",
        "\n",
        "-   **While R-squared is a valuable metric, it has limitations:**\n",
        "Not Always Indicative of Model Quality: A high R-squared does not necessarily mean the model is good; it may indicate overfitting, where the model captures noise rather than the underlying relationship.\n",
        "\n",
        "\n",
        "-   **Ignores Model Complexity:** R-squared does not account for the number of predictors in the model, which can lead to misleading interpretations if used in isolation.\n",
        "\n",
        "\n",
        "\n",
        "In summary, R-squared is a fundamental metric in regression analysis that helps evaluate model performance, interpret variance, and compare different models, but it should be used alongside other metrics and diagnostic tools for a comprehensive assessment of model quality."
      ],
      "metadata": {
        "id": "8sV8V7-7sHi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 9.Write Python code to fit a simple linear regression model using scikit-learn\n",
        "and print the slope and intercept."
      ],
      "metadata": {
        "id": "yAmwVt__tM8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data (independent variable X and dependent variable y)\n",
        "# X must be 2D array for scikit-learn\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "y = np.array([2, 4, 5, 4, 5])\n",
        "\n",
        "# Create Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get slope (coefficient) and intercept\n",
        "slope = model.coef_[0]\n",
        "intercept = model.intercept_\n",
        "\n",
        "# Print results\n",
        "print(\"Slope (Coefficient):\", slope)\n",
        "print(\"Intercept:\", intercept)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kOKBw4qteJH",
        "outputId": "414c9f45-b77b-44d6-bcf2-08d826ab2f31"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Slope (Coefficient): 0.6\n",
            "Intercept: 2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 10.How do you interpret the coefficients in a simple linear regression model?\n",
        "\n",
        "Ans--When given a computer-generated regression model, the goal is to understand how each independent variable influences the dependent variable. A typical multiple linear regression equation looks like:\n",
        "\n",
        "[ Y = β_0 + β_1X_1 + β_2X_2 + ... + β_nX_n + ϵ ]\n",
        "\n",
        "Here, β₀ is the intercept, βᵢ are the coefficients, and ϵ is the error term.\n",
        "\n",
        "1. Intercept (β₀) Represents the expected value of Y when all independent variables are zero. While it sets the baseline, it may not always have a practical meaning if zero values are unrealistic.\n",
        "\n",
        "2. Coefficients (βᵢ)\n",
        "\n",
        "Sign: Positive → Direct relationship (X increases → Y increases). Negative → Inverse relationship (X increases → Y decreases).\n",
        "\n",
        "Magnitude: The absolute value shows the strength of the effect.\n",
        "\n",
        "Statistical Significance: Check p-values (commonly p < 0.05) to see if the effect is meaningful.\n",
        "\n",
        "Confidence Intervals: Narrow intervals mean more precise estimates.\n",
        "\n",
        "3. Example For a housing price model: [ Price = 50000 + 300 \\times Size + 10000 \\times Bedrooms - 2000 \\times Age ]\n",
        "\n",
        "Intercept = 50000 → Base price when all predictors are zero.\n",
        "\n",
        "Size = 300 → Each extra square foot adds $300.\n",
        "\n",
        "Bedrooms = 10000 → Each extra bedroom adds $10,000.\n",
        "\n",
        "Age = -2000 → Each year reduces price by $2,000.\n",
        "\n",
        "below are given python code\n"
      ],
      "metadata": {
        "id": "U-MZ_bW2tlay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "data = {\n",
        "   'Price': [200000, 250000, 300000, 350000, 400000],\n",
        "   'SquareFootage': [1500, 2000, 2500, 3000, 3500],\n",
        "   'Bedrooms': [3, 4, 3, 5, 4]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "X = sm.add_constant(df[['SquareFootage', 'Bedrooms']])\n",
        "y = df['Price']\n",
        "model = sm.OLS(y, X).fit()\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScdAAKdkuCxD",
        "outputId": "02ebab00-be73-479a-c1b1-67d454b836ce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            OLS Regression Results                            \n",
            "==============================================================================\n",
            "Dep. Variable:                  Price   R-squared:                       1.000\n",
            "Model:                            OLS   Adj. R-squared:                  1.000\n",
            "Method:                 Least Squares   F-statistic:                 8.758e+28\n",
            "Date:                Fri, 13 Feb 2026   Prob (F-statistic):           1.14e-29\n",
            "Time:                        11:17:23   Log-Likelihood:                 103.68\n",
            "No. Observations:                   5   AIC:                            -201.4\n",
            "Df Residuals:                       2   BIC:                            -202.5\n",
            "Df Model:                           2                                         \n",
            "Covariance Type:            nonrobust                                         \n",
            "=================================================================================\n",
            "                    coef    std err          t      P>|t|      [0.025      0.975]\n",
            "---------------------------------------------------------------------------------\n",
            "const              5e+04   8.85e-10   5.65e+13      0.000       5e+04       5e+04\n",
            "SquareFootage   100.0000    2.9e-13   3.45e+14      0.000     100.000     100.000\n",
            "Bedrooms      -1.409e-11   2.74e-10     -0.051      0.964   -1.19e-09    1.17e-09\n",
            "==============================================================================\n",
            "Omnibus:                          nan   Durbin-Watson:                   0.006\n",
            "Prob(Omnibus):                    nan   Jarque-Bera (JB):                1.888\n",
            "Skew:                           1.500   Prob(JB):                        0.389\n",
            "Kurtosis:                       3.250   Cond. No.                     1.39e+04\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "[2] The condition number is large, 1.39e+04. This might indicate that there are\n",
            "strong multicollinearity or other numerical problems.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/statsmodels/stats/stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 5 samples were given.\n",
            "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n"
          ]
        }
      ]
    }
  ]
}