{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Trees, SVM, and Naive Bayes"
      ],
      "metadata": {
        "id": "6okJalppHNK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 1.What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Answer: Information Gain (IG) is a metric used in Decision Trees to measure how much uncertainty (entropy) is reduced after splitting a dataset on a particular feature.\n",
        "\n",
        "It is based on the concept of Entropy, which measures impurity in a dataset.\n",
        "\n",
        "ùê∏\n",
        "ùëõ\n",
        "ùë°\n",
        "ùëü\n",
        "ùëú\n",
        "ùëù\n",
        "ùë¶\n",
        "(\n",
        "ùëÜ\n",
        ")=\n",
        "‚àí\n",
        "‚àë\n",
        "ùëù\n",
        "<sub>ùëñ</sub>\n",
        "log<sub>2</sub>(p<sub>i</sub>)\n",
        "\n",
        "Where:\n",
        "\n",
        "-   p<sub>ùëñ</sub>= probability of class i\n",
        "\n",
        "Information Gain is calculated as:\n",
        "\n",
        "IG(S,A)=Entropy(S)‚àí‚àë\n",
        "‚à£S <sub> v </sub>‚à£ / |s| Entropy(S <sub>v</sub>)\n",
        "\n",
        "Where:\n",
        "\n",
        "-  S = dataset\n",
        "-  A = feature\n",
        "\n",
        "-  ùëÜ<sub>ùë£</sub>= subset after split\n",
        "\n",
        "How it is used:\n",
        "\n",
        "-  For every feature, Information Gain is calculated.\n",
        "\n",
        "-  The feature with the highest Information Gain is selected for splitting.\n",
        "\n",
        "-  This process repeats recursively.\n",
        "\n",
        "Importance:\n",
        "\n",
        "-  Helps build an optimal tree.\n",
        "\n",
        "-  Reduces randomness in splits.\n",
        "\n",
        "-  Used in algorithms like ID3 and C4.5."
      ],
      "metadata": {
        "id": "VvkMSbYrHQBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 2.Difference Between Gini Impurity and Entropy.\n",
        "\n",
        "Ans--\n",
        "\n",
        "\n",
        "| Basis          | Gini Impurity                    | Entropy                           |\n",
        "| -------------- | -------------------------------- | --------------------------------- |\n",
        "| Formula        | (1 - \\sum p_i^2)                 | (-\\sum p_i \\log_2 p_i)            |\n",
        "| Range          | 0 to 0.5 (binary)                | 0 to 1                            |\n",
        "| Speed          | Faster to compute                | Slightly slower (log calculation) |\n",
        "| Interpretation | Probability of misclassification | Measure of information disorder   |\n",
        "| Used in        | CART Algorithm                   | ID3, C4.5                         |\n",
        "\n",
        "Key Differences:\n",
        "\n",
        "-  Gini is computationally efficient.\n",
        "\n",
        "-  Entropy has stronger theoretical foundation (Information Theory).\n",
        "\n",
        "-  In practice, both give similar results."
      ],
      "metadata": {
        "id": "qOtkDroYKouf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 3: What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Pre-Pruning (Early Stopping) is a technique used to stop the growth of a Decision Tree before it becomes too complex.\n",
        "\n",
        "Purpose:\n",
        "\n",
        "To prevent overfitting.\n",
        "\n",
        "**Common Pre-Pruning Techniques:**\n",
        "\n",
        "-  Setting max_depth\n",
        "\n",
        "-  Setting min_samples_split\n",
        "\n",
        "-  Setting min_samples_leaf\n",
        "\n",
        "-  Setting minimum information gain threshold\n",
        "\n",
        "Advantages:\n",
        "\n",
        "-  Reduces model complexity\n",
        "\n",
        "-  Faster training\n",
        "\n",
        "-  Better generalization\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "-  May stop too early (underfitting)"
      ],
      "metadata": {
        "id": "4IBLNevFLIhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 4.:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances."
      ],
      "metadata": {
        "id": "A3GFguvOLvNa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BNTSzwHvHJG9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a420d62-6334-40b2-d946-31db82e8beb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree with Gini\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(data.feature_names, model.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 5.What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression.\n",
        "\n",
        "Core Idea:\n",
        "\n",
        "Find the optimal hyperplane that separates classes with the maximum margin.\n",
        "\n",
        "Key Concepts:\n",
        "\n",
        "-  Hyperplane\n",
        "\n",
        "-  Margin\n",
        "\n",
        "-   Vectors (data points closest to hyperplane)\n",
        "\n",
        "Advantages:\n",
        "\n",
        "-  Works well with high-dimensional data\n",
        "\n",
        "-  Effective in complex classification"
      ],
      "metadata": {
        "id": "Fbyhc_o0MCRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 6.What is the Kernel Trick in SVM?\n",
        "\n",
        "Answer:\n",
        "\n",
        "The Kernel Trick allows SVM to perform classification in higher-dimensional space without explicitly computing coordinates in that space.\n",
        "\n",
        "Instead of transforming data manually, a kernel function computes similarity directly.\n",
        "\n",
        "Common Kernels:\n",
        "\n",
        "-  Linear\n",
        "\n",
        "-  Polynomial\n",
        "\n",
        "-   Radial Basis Function (RBF)\n",
        "\n",
        "-    Sigmoid\n",
        "\n",
        "Benefit:\n",
        "\n",
        "-  Allows SVM to solve non-linear problems efficiently."
      ],
      "metadata": {
        "id": "DebCe3o6MUgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 7.Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n",
        "\n"
      ],
      "metadata": {
        "id": "YtCrckieMgvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Linear SVM\n",
        "linear_svm = SVC(kernel='linear')\n",
        "linear_svm.fit(X_train, y_train)\n",
        "linear_pred = linear_svm.predict(X_test)\n",
        "linear_acc = accuracy_score(y_test, linear_pred)\n",
        "\n",
        "# RBF SVM\n",
        "rbf_svm = SVC(kernel='rbf')\n",
        "rbf_svm.fit(X_train, y_train)\n",
        "rbf_pred = rbf_svm.predict(X_test)\n",
        "rbf_acc = accuracy_score(y_test, rbf_pred)\n",
        "\n",
        "print(\"Linear Kernel Accuracy:\", linear_acc)\n",
        "print(\"RBF Kernel Accuracy:\", rbf_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e3UdB6SMlTm",
        "outputId": "07d04c4d-5853-4eef-81f1-ba8885d439e6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 0.9814814814814815\n",
            "RBF Kernel Accuracy: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 8.What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?\n",
        "\n",
        "Ans--Na√Øve Bayes is a probabilistic classification algorithm based on Bayes‚Äô Theorem.\n",
        "\n",
        "P(A‚à£B)=\n",
        "P(B‚à£A)P(A)/P(B)\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "It assumes that all features are independent of each other.\n",
        "\n",
        "**Why \"Na√Øve\"?**\n",
        "\n",
        "-  Because it makes a strong independence assumption, which is rarely true in real-world data.\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "-  Fast\n",
        "\n",
        "-  Works well with large datasets\n",
        "\n",
        "-  Good for text classification"
      ],
      "metadata": {
        "id": "ZOBBhqqaNejK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 9.Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve\n",
        "Bayes, and Bernoulli Na√Øve Bayes.\n",
        "\n",
        "Ans--\n",
        "| Type           | Used For            | Data Type            |\n",
        "| -------------- | ------------------- | -------------------- |\n",
        "| Gaussian NB    | Continuous data     | Normally distributed |\n",
        "| Multinomial NB | Text classification | Discrete counts      |\n",
        "| Bernoulli NB   | Binary features     | 0/1 features         |\n",
        "\n",
        "**Gaussian NB**\n",
        "\n",
        "-  Assumes normal distribution.\n",
        "\n",
        "**Multinomial NB**\n",
        "\n",
        "-  Used for word frequency.\n",
        "\n",
        "**Bernoulli NB**\n",
        "\n",
        "-  Used for binary presence/absence features."
      ],
      "metadata": {
        "id": "9oHXzL-vOTDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 10. Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy."
      ],
      "metadata": {
        "id": "MOWao_URO6eP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruuZVQ0uO8XQ",
        "outputId": "6df790ac-9d8e-471f-a9ba-30722f8c1838"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9415204678362573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VsY7V9AKNeSI"
      }
    }
  ]
}