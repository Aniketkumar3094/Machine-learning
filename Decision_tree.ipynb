{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Decision tree\n"
      ],
      "metadata": {
        "id": "bV6-6CXo1Gge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 1.What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "Ans--A Decision Tree is a supervised learning algorithm that mimics human decision-making by breaking down a dataset into smaller subsets based on feature values. It consists of nodes and branches that represent decisions and their possible consequences. The main components of a Decision Tree include:\n",
        "\n",
        "-   **Root Node:** Represents the entire dataset and the first decision point.\n",
        "\n",
        "-   **Decision Nodes:** Internal nodes that represent tests on attributes (features) of the data.\n",
        "\n",
        "-   **Leaf Nodes:** Terminal nodes that provide the final output or class label (e.g., \"spam\" or \"not spam\").\n",
        "\n",
        "-   **Branches:** Lines connecting nodes, representing the outcome of a test and leading to the next node or leaf."
      ],
      "metadata": {
        "id": "7xAqxNoK1OoR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 2. Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Ans--**Definition:** Gini Impurity measures the likelihood of misclassifying a randomly chosen element from the dataset. It quantifies the probability of a data point being incorrectly labeled if it were randomly assigned a label according to the distribution of labels in the subset.\n",
        "\n",
        "**Formula:** The Gini Impurity G for a node is calculated as:\n",
        "\n",
        "\\[ G = 1 - \\sum_{i=1}\\^{c} p_i\\^2 \\]\n",
        "\n",
        "where\n",
        "p<sub>i</sub>\n",
        "  is the probability of a data point belonging to class\n",
        "i and\n",
        "c is the number of classes. The Gini value ranges from 0 (pure node) to 0.5 (maximum impurity for binary classification).\n",
        "\n",
        "-  **Impact on Splits:** When building a decision tree, the algorithm selects the feature that results in the lowest Gini Impurity for the child nodes after the split. This helps create more homogeneous groups, improving the model's predictive power.\n",
        "\n",
        "**Entropy**\n",
        "-  **Definition:** Entropy is a measure of the disorder or uncertainty in a dataset. It quantifies the impurity of a node by measuring the unpredictability of the class labels in that node.\n",
        "\n",
        "**Formula:** The Entropy H for a node is calculated as:\n",
        "\n",
        "-     \\[ H = -\\sum_{i=1}\\^{c} p_i \\log_2(p_i) \\]\n",
        "where\n",
        "p<sub>i</sub> is the probability of a data point belonging to class\n",
        "i. Entropy values range from 0 (pure node) to\n",
        "log<sub>2</sub>(c)(maximum impurity).\n",
        "\n",
        "-   **Impact on Splits:** Similar to Gini Impurity, the decision tree algorithm uses Entropy to determine the best feature for splitting. The feature that results in the highest information gain (the reduction in entropy) is chosen for the split, leading to more informative and effective decision-making.\n",
        "\n",
        " **Comparison and Practical Implications**\n",
        "\n",
        "-  **Computational Efficiency:** Gini Impurity is generally faster to compute than Entropy because it does not involve logarithmic calculations. This can make Gini a preferred choice in many practical implementations of decision trees.\n",
        "\n",
        "-  **Performance:** While both measures often yield similar results, the choice between Gini Impurity and Entropy can depend on the specific dataset and problem context. Gini tends to be more sensitive to class distribution, while Entropy provides a more comprehensive view of the uncertainty in the dataset.\n",
        "\n",
        "In summary, both Gini Impurity and Entropy are essential for guiding the splits in decision trees, helping to create models that are both accurate and efficient in classifying data. Understanding these concepts is crucial for anyone working with decision trees in machine learning."
      ],
      "metadata": {
        "id": "fM4Yc0At1urP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 3.What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "\n",
        "Ans--**1.Pre-Pruning (Early Stopping)**\n",
        "\n",
        "-   **Definition:** Pre-pruning stops the tree growth during training before it becomes fully complex. The tree does not grow beyond a certain point if further splits do not meet a predefined criterion. Common criteria include minimum information gain, minimum samples per node, maximum depth, or statistical significance tests (like chi-square).\n",
        "\n",
        "-   **Key Features:**\n",
        "Halts the creation of nodes while building the tree.\n",
        "Prevents creation of branches that do not significantly improve performance on the training set.\n",
        "\n",
        "-   **Practical Advantage:**\n",
        "Reduces overfitting early and saves computational resources because fewer nodes are created and evaluated. This is especially useful for very large datasets where a fully grown tree would be expensive to build.\n",
        "\n",
        "**2. Post-Pruning (Cost-Complexity Pruning / Reduced-Error Pruning)**\n",
        "\n",
        "-   **Definition:** Post-pruning allows the tree to be fully grown first and then prunes back nodes that do not contribute significantly to predictive accuracy based on a validation set or a complexity measure. Techniques include cost-complexity pruning (CART) or validating on a separate data set.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "Initially builds a potentially overfitted tree capturing all patterns in the training data.\n",
        "Iteratively removes branches/nodes that add minimal predictive value, simplifying the tree.\n",
        "\n",
        "**Practical Advantage:**\n",
        "Achieves higher predictive accuracy on unseen data because the full tree captures complex relationships before pruning, allowing smarter decisions about which branches are truly redun"
      ],
      "metadata": {
        "id": "Syci9-x6tO8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 4.What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "Ans--Information Gain (IG) is a metric used in Decision Trees to measure how much uncertainty (impurity) in the target variable is reduced after splitting the dataset based on a particular feature.\n",
        "\n",
        "It is based on the concept of Entropy from information theory.\n",
        "\n",
        "Information Gain is calculated as\n",
        "\n",
        "IG=Entropy(parent)−∑(\n",
        "N<sub>parent</sub>/\n",
        "N<sub>child</sub> × Entropy(child))\n",
        "\n",
        "Where:\n",
        "\n",
        "-  Entropy(parent) = impurity before split\n",
        "\n",
        "-  Entropy(child) = impurity after split\n",
        "\n",
        "Weighted sum accounts for proportion of samples in each child node\n",
        "\n",
        "**Why Information Gain is Important**\n",
        "\n",
        "\n",
        "-  Information Gain is used to choose the best feature for splitting at each node.\n",
        "\n",
        "**The Decision Tree algorithm:**\n",
        "\n",
        "\n",
        "-   Computes Information Gain for all possible features.\n",
        "\n",
        "-   Selects the feature with the highest Information Gain.\n",
        "\n",
        "-   Performs the split.\n",
        "\n",
        "-   Repeats recursively.\n",
        "\n",
        "**Reason:**\n",
        "\n",
        "-  The feature with the highest IG:\n",
        "\n",
        "-  Produces the purest child nodes\n",
        "\n",
        "-   Reduces uncertainty the most\n",
        "\n",
        "-  Improves classification accuracy"
      ],
      "metadata": {
        "id": "FDy4wp6fve6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 5.What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "Ans--**Decision Trees are widely used in real-world applications such as finance, healthcare, and marketing due to their interpretability, simplicity, and versatility.**\n",
        "**Common Applications**\n",
        "-   1. Finance and Banking:\n",
        "Decision Trees are used for credit risk analysis, loan approval, and fraud detection. They help in classifying applicants as low or high risk based on various financial and demographic features.\n",
        "\n",
        "-   2. Healthcare and Medicine\n",
        "In medical diagnostics, Decision Trees assist in disease prediction and patient outcome analysis. For example, physicians can predict the likelihood of illnesses like diabetes or heart disease based on patient data.\n",
        "\n",
        "-   3. Marketing and Customer Analytics\n",
        "Businesses use Decision Trees to segment customers, predict customer churn, or recommend products. They classify customers into groups based on purchasing behavior, demographics, and engagement history.\n",
        "\n",
        "**Main Advantages**\n",
        "\n",
        "-  Interpretability and Transparency:Each decision is represented as a simple rule in the tree, making it easy for humans to understand the reasoning behind predictions.\n",
        "-  Handling of Both Numerical and Categorical Data:\n",
        "Decision Trees can manage diverse data types without extensive preprocessing.\n",
        "-  Nonparametric Nature:\n",
        "They do not assume any underlying distribution of data, allowing them to capture complex patterns and relationships.\n",
        "-   Automatic Feature Selection:By evaluating splits based on information gain or Gini impurity, Decision Trees inherently highlight the most important variables.\n",
        "-   Ease of Use and Visualization:Their structure is intuitive and can be visualized clearly, making them suitable for decision-making and communication with non-technical stakeholders.\n",
        "-   Versatility:They can be used for classification, regression, and even multi-output predictions, making them broadly applicable across domains.\n",
        "\n",
        "Overall, Decision Trees offer a powerful combination of clarity, flexibility, and practical utility across diverse industries, making them a popular choice for both predictive modeling and prescriptive analytics."
      ],
      "metadata": {
        "id": "YQyKI-lJyqF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 6.Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "MPYYNK0M0Di0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Feature importance\n",
        "print(\"Feature Importances:\", clf.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHl8zaTb0obt",
        "outputId": "bd3ef799-7083-456f-f4b1-bf62739b60a4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances: [0.         0.01911002 0.89326355 0.08762643]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 7.Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree."
      ],
      "metadata": {
        "id": "CCv_3LsN0z8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fully grown tree\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "full_acc = accuracy_score(y_test, full_tree.predict(X_test))\n",
        "\n",
        "# Limited depth tree\n",
        "limited_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "limited_tree.fit(X_train, y_train)\n",
        "limited_acc = accuracy_score(y_test, limited_tree.predict(X_test))\n",
        "\n",
        "print(\"Fully Grown Tree Accuracy:\", full_acc)\n",
        "print(\"Max Depth=3 Accuracy:\", limited_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvV7G4Q406YD",
        "outputId": "ebd23fca-3228-4e2c-f56b-622d7b19e65d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully Grown Tree Accuracy: 1.0\n",
            "Max Depth=3 Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 8. Write a Python program to:\n",
        "● Load the California Housing dataset from sklearn\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "z1KSIR0C1Av-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "reg = DecisionTreeRegressor(random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "# MSE\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"Feature Importances:\", reg.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqoVSEBP1JYo",
        "outputId": "46630def-88be-487c-ecf6-7a653e8f9721"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.5280096503174904\n",
            "Feature Importances: [0.52345628 0.05213495 0.04941775 0.02497426 0.03220553 0.13901245\n",
            " 0.08999238 0.08880639]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 9.Write a Python program to:\n",
        "\n",
        "● Load the Iris Dataset\n",
        "\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "\n",
        "● Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "9p4ZAVi-1QV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Load Iris dataset (as per Q9)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data (as per previous examples)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
        "                    param_grid,\n",
        "                    cv=5)\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Accuracy:\", grid.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvIArTZZ1ZtB",
        "outputId": "21ad9ec5-cc9c-4119-f20f-c433294e64e6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Best Accuracy: 0.9428571428571428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q 10.Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "● Handle the missing values\n",
        "\n",
        "● Encode the categorical features\n",
        "\n",
        "● Train a Decision Tree model\n",
        "\n",
        "● Tune its hyperparameters\n",
        "\n",
        "● Evaluate its performance\n",
        "\n",
        "And describe what business value this model could provide in the real-world setting.\n",
        "\n",
        "\n",
        "\n",
        "Ans--\n",
        "step 1: **Handle Missing Values**\n",
        "\n",
        "\n",
        "-  Numerical → Mean/Median imputation\n",
        "\n",
        "-  Categorical → Mode imputation\n",
        "\n",
        "-  Use SimpleImputer\n",
        "\n",
        "Step 2: **Encode Categorical Features**\n",
        "\n",
        "-  Nominal → One-Hot Encoding\n",
        "\n",
        "-  Ordinal → Label Encoding\n",
        "\n",
        "Step 3: **Train Decision Tree Model**\n",
        "\n",
        "-  Split data (train/test)\n",
        "\n",
        "-  Use DecisionTreeClassifier\n",
        "\n",
        "-  Select impurity criterion\n",
        "\n",
        "Step 4: **Hyperparameter Tuning**\n",
        "\n",
        "-  Tune:\n",
        "\n",
        "   -    max_depth\n",
        "\n",
        "   -    min_samples_split\n",
        "\n",
        "   -    min_samples_leaf\n",
        "\n",
        "-   Use GridSearchCV\n",
        "\n",
        "-   Apply cross-validation\n",
        "\n",
        "Step 5: **Evaluate Performance**\n",
        "\n",
        "-  Accuracy\n",
        "\n",
        "-  Precision\n",
        "\n",
        "-   Recall\n",
        "\n",
        "-  F1-score\n",
        "\n",
        "-  Confusion Matrix\n",
        "\n",
        "-  ROC-AUC\n",
        "\n",
        "**Business Value**\n",
        "\n",
        "-  Early disease detection\n",
        "\n",
        "-  Reduced healthcare costs\n",
        "\n",
        "-  Faster diagnosis\n",
        "\n",
        "-  Better treatment prioritization\n",
        "\n",
        "-  Improved patient survival rates\n",
        "\n",
        "A well-tuned Decision Tree model can assist doctors in decision support systems and risk stratification."
      ],
      "metadata": {
        "id": "sCn6SOdX10X3"
      }
    }
  ]
}