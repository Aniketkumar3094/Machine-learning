{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble Learning"
      ],
      "metadata": {
        "id": "a8MN4j72RbNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1.What is Ensemble Learning in machine learning? Explain the key idea behind it.**\n",
        "\n",
        "\n",
        "Answer:\n",
        "\n",
        "Ensemble Learning is a machine learning paradigm where multiple base models (also called weak learners) are combined to produce a stronger and more robust predictive model. Instead of relying on a single model, ensemble methods aggregate predictions from several models to improve generalization performance.\n",
        "\n",
        "Key Idea Behind Ensemble Learning\n",
        "\n",
        "The fundamental principle is:\n",
        "\n",
        "    ‚ÄúA group of weak learners can come together to form a strong learner.‚Äù\n",
        "\n",
        "The main objectives are:\n",
        "\n",
        "-  Reduce variance\n",
        "\n",
        "-  Reduce bias\n",
        "\n",
        "-  Improve predictive accuracy\n",
        "\n",
        "-  Increase robustness against noise\n",
        "\n",
        "Mathematical Perspective\n",
        "\n",
        "If we denote individual models as:\n",
        "\n",
        "‚Ñé\n",
        "1\n",
        "(\n",
        "ùë•\n",
        ")\n",
        ",\n",
        "‚Ñé\n",
        "2\n",
        "(\n",
        "ùë•\n",
        ")\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "‚Ñé\n",
        "ùëõ\n",
        "(\n",
        "ùë•\n",
        ")\n",
        "\n",
        "\n",
        "The ensemble prediction is typically:\n",
        "\n",
        "-  Classification (Voting):\n",
        "\n",
        "  H(x)=majority vote(h<sub>1</sub>(x),...,h<sub>n</sub>(x))\n",
        "\n",
        "-  Regression (Averaging):\n",
        "\n",
        "H(x)=1/n i=‚àëh<sub>i</sub>(x)\n",
        "\n",
        "**Why It Works**\n",
        "\n",
        "Errors made by individual models may cancel each other out if:\n",
        "\n",
        "-  Models are diverse\n",
        "\n",
        "-  Errors are uncorrelated\n",
        "\n",
        "**Types of Ensemble Methods**\n",
        "\n",
        "1. Bagging (Bootstrap Aggregating)\n",
        "\n",
        "2. Boosting\n",
        "\n",
        "3. Stacking\n",
        "\n",
        "**Advantages**\n",
        "\n",
        "-  Higher accuracy\n",
        "\n",
        "-  Better generalization\n",
        "\n",
        "-  Reduced overfitting (in bagging)\n",
        "\n",
        "-  Strong performance in competitions\n",
        "\n",
        "**Disadvantages**\n",
        "\n",
        "-  Increased computational cost\n",
        "\n",
        "-  Reduced interpretability\n",
        "\n",
        "-  Complex tuning\n",
        "\n",
        "**Question 2: What is the difference between Bagging and Boosting?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Bagging and Boosting are two primary ensemble techniques but differ significantly in methodology.\n",
        "**Comparison Table**\n",
        "\n",
        "| Aspect           | Bagging                               | Boosting                     |\n",
        "| ---------------- | ------------------------------------- | ---------------------------- |\n",
        "| Full Form        | Bootstrap Aggregating                 | Sequential Weight Updating   |\n",
        "| Training Style   | Parallel                              | Sequential                   |\n",
        "| Data Sampling    | Bootstrap sampling (with replacement) | Reweighting data points      |\n",
        "| Goal             | Reduce variance                       | Reduce bias                  |\n",
        "| Model Dependency | Independent models                    | Dependent models             |\n",
        "| Example          | Random Forest                         | AdaBoost, Gradient Boosting  |\n",
        "| Overfitting Risk | Low                                   | Higher if not tuned properly |\n",
        "\n",
        "\n",
        "**Bagging**\n",
        "\n",
        "-  Each model is trained on a random bootstrap sample.\n",
        "\n",
        "-  Predictions are aggregated using voting/averaging.\n",
        "\n",
        "-  Works well with high-variance models (e.g., Decision Trees).\n",
        "\n",
        "-  Reduces variance:\n",
        "\n",
        "Var<sub>ensemble</sub>=1/nVar<sub>individual</sub>\n",
        "\t‚Äã\n",
        "\n",
        "**Boosting**\n",
        "\n",
        "-  Models are trained sequentially.\n",
        "\n",
        "-  Each new model focuses on previous errors.\n",
        "\n",
        "-  Assigns higher weights to misclassified samples.\n",
        "\n",
        "Boosting reduces bias by combining weak learners into a strong learner.\n",
        "\n",
        "\n",
        "**Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?**\n",
        "\n",
        "Answer:\n",
        "\n",
        "Bootstrap sampling is a statistical resampling technique where samples are drawn with replacement from the original dataset.\n",
        "\n",
        "**Process**\n",
        "\n",
        "Given dataset size N:\n",
        "\n",
        "-  Randomly sample N observations with replacement\n",
        "\n",
        "-  Some samples appear multiple times\n",
        "\n",
        "-  Some samples are not selected\n",
        "\n",
        "Approximately:\n",
        "\n",
        "-  63% of data is selected\n",
        "\n",
        "-  37% remains unused (OOB samples)\n",
        "\n",
        "**Role in Bagging**\n",
        "\n",
        "In Bagging:\n",
        "\n",
        "-  Each base learner is trained on a different bootstrap sample.\n",
        "\n",
        "-  Ensures model diversity.\n",
        "\n",
        "-  Reduces correlation between trees.\n",
        "\n",
        "**In Random Forest**\n",
        "\n",
        "Random Forest extends bootstrap sampling by:\n",
        "\n",
        "1. Bootstrap sampling of rows\n",
        "\n",
        "2. Random selection of features at each split\n",
        "\n",
        "\n",
        "This double randomness:\n",
        "\n",
        "\n",
        "-  Increases diversity\n",
        "\n",
        "-  Reduces overfitting\n",
        "\n",
        "-  Improves stability\n",
        "\n",
        "\n",
        "**Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?**\n",
        "\n",
        "\n",
        "Answer:\n",
        "\n",
        "\n",
        "Out-of-Bag (OOB) samples are the observations not selected in a bootstrap sample.\n",
        "\n",
        "\n",
        "Since each tree sees only about 63% of the data:\n",
        "\n",
        "\n",
        "-  The remaining 37% is OOB for that tree.\n",
        "\n",
        "**How OOB Score Works**\n",
        "\n",
        "1. For each sample:\n",
        "\n",
        "    -   Identify trees where it was OOB\n",
        "\n",
        "2. Predict using only those trees\n",
        "\n",
        "3. Compare prediction with actual label\n",
        "\n",
        "\n",
        "OOB score ‚âà validation accuracy without needing separate validation set.\n",
        "\n",
        "**Advantages**\n",
        "\n",
        "-  No need for train-test split\n",
        "\n",
        "-  Efficient use of data\n",
        "\n",
        "-  Internal validation mechanism\n",
        "\n",
        "\n",
        "**Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.**\n",
        "\n",
        "Answer:\n",
        "\n",
        "**Single Decision Tree**\n",
        "\n",
        "\n",
        "Feature importance is calculated based on:\n",
        "\n",
        "-  Reduction in impurity (Gini/Entropy)\n",
        "\n",
        "-  Contribution to splits\n",
        "\n",
        "Limitations:\n",
        "\n",
        "\n",
        "-  High variance\n",
        "\n",
        "-  Sensitive to noise\n",
        "\n",
        "-  May overestimate importance of dominant features\n",
        "\n",
        "\n",
        "**Random Forest**\n",
        "\n",
        "\n",
        "Feature importance is averaged across many trees:\n",
        "\n",
        "Importance(f)=1/T‚àët=1I mportance<sub>t</sub>(f)\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "-  More stable\n",
        "\n",
        "-  Less biased\n",
        "\n",
        "-  Robust to noise\n",
        "\n",
        "**Comparison**\n",
        "\n",
        "| Factor      | Decision Tree | Random Forest |\n",
        "| ----------- | ------------- | ------------- |\n",
        "| Stability   | Low           | High          |\n",
        "| Bias        | High          | Lower         |\n",
        "| Variance    | High          | Reduced       |\n",
        "| Reliability | Moderate      | Strong        |\n",
        "\n",
        "\n",
        "Random Forest gives more reliable feature ranking."
      ],
      "metadata": {
        "id": "whiyNIUnRiuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6.Write a Python program to:\n",
        "\n",
        "‚óè Load the Breast Cancer dataset using\n",
        "\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "‚óè Train a Random Forest Classifier\n",
        "\n",
        "‚óè Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "lzKbffClv1lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Feature importance\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create DataFrame\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort and get top 5\n",
        "top5 = feature_importance_df.sort_values(by='Importance', ascending=False).head(5)\n",
        "\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(top5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8DRFscOwd8i",
        "outputId": "869dbe0e-bcfe-4e7d-d7ce-9eea98e676a9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "                 Feature  Importance\n",
            "7    mean concave points    0.141934\n",
            "27  worst concave points    0.127136\n",
            "23            worst area    0.118217\n",
            "6         mean concavity    0.080557\n",
            "20          worst radius    0.077975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7. Write a Python program to:\n",
        "\n",
        "‚óè Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "\n",
        "‚óè Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "92oF3BtywnEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# Bagging Classifier\n",
        "bag = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag.fit(X_train, y_train)\n",
        "bag_pred = bag.predict(X_test)\n",
        "bag_accuracy = accuracy_score(y_test, bag_pred)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_accuracy)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCcZllizwxQd",
        "outputId": "4a89b144-b3e1-4ac6-d6be-92416bb8e5f8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8.Write a Python program to:\n",
        "\n",
        "‚óè Train a Random Forest Classifier\n",
        "\n",
        "‚óè Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "\n",
        "‚óè Print the best parameters and final accuracy.\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "Q2yFZcEJw4i1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 5, 10, 15]\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# Evaluate best model\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Final Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLhrFh2ZxWam",
        "outputId": "1abb18a6-1812-401b-ad28-8a446485d216"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 5, 'n_estimators': 150}\n",
            "Final Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9.Write a Python program to:\n",
        "‚óè Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "‚óè Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "Answer:\n",
        "\n"
      ],
      "metadata": {
        "id": "FUMPEsKBxfi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging Regressor\n",
        "bag_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bag_reg.fit(X_train, y_train)\n",
        "bag_pred = bag_reg.predict(X_test)\n",
        "bag_mse = mean_squared_error(y_test, bag_pred)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "print(\"Bagging Regressor MSE:\", bag_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmoB7EQWxm6M",
        "outputId": "0143dc90-afde-4d55-c439-fec15e8de6bc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.25787382250585034\n",
            "Random Forest Regressor MSE: 0.25650512920799395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10.You are working as a data scientist at a financial institution to predict loan\n",
        "default.\n",
        "You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "\n",
        "‚óè Choose between Bagging or Boosting\n",
        "\n",
        "‚óè Handle overfitting\n",
        "\n",
        "‚óè Select base models\n",
        "\n",
        "‚óè Evaluate performance using cross-validation\n",
        "\n",
        "‚óè Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "\n",
        "Answer:\n",
        "\n",
        "**Step-by-Step Approach**\n",
        "\n",
        "1. Choose Between Bagging or Boosting\n",
        "\n",
        "-  If high variance problem ‚Üí Bagging (Random Forest)\n",
        "\n",
        "-  If high bias problem ‚Üí Boosting (Gradient Boosting, XGBoost)\n",
        "\n",
        "For loan default:\n",
        "\n",
        "-  Boosting often preferred due to complex patterns.\n",
        "\n",
        "2. Handle Overfitting\n",
        "\n",
        "\n",
        "Techniques:\n",
        "\n",
        "\n",
        "-  Limit max_depth\n",
        "\n",
        "-  Use regularization\n",
        "\n",
        "-  Use early stopping\n",
        "\n",
        "-  Cross-validation\n",
        "\n",
        "-  Feature selection\n",
        "\n",
        "3. Select Base Models\n",
        "\n",
        "\n",
        "Common choices:\n",
        "\n",
        "\n",
        "-  Decision Trees (weak learners)\n",
        "\n",
        "-  Logistic Regression (interpretable)\n",
        "\n",
        "-  Gradient Boosting models\n",
        "\n",
        "In financial context:\n",
        "\n",
        "\n",
        "-  Decision Trees preferred due to interpretability.\n",
        "\n",
        "4. Evaluate Performance Using Cross-Validation\n",
        "\n",
        "\n",
        "Use:\n",
        "\n",
        "\n",
        "-  k-Fold Cross Validation (k=5 or 10)\n",
        "\n",
        "-  Stratified sampling (due to class imbalance)\n",
        "\n",
        "    Evaluation metrics:\n",
        "\n",
        "\n",
        "-  Accuracy\n",
        "\n",
        "-  Precision\n",
        "\n",
        "-  Recall\n",
        "\n",
        "-  F1-score\n",
        "\n",
        "-  ROC-AUC (important in loan default)\n",
        "\n",
        "5. Justification for Ensemble Learning\n",
        "\n",
        "\n",
        "    Loan default prediction involves:\n",
        "\n",
        "\n",
        "-  High dimensional data\n",
        "\n",
        "-  Noisy financial records\n",
        "\n",
        "-  Non-linear relationships\n",
        "\n",
        "    **Ensemble learning:**\n",
        "\n",
        "\n",
        "-  Improves predictive stability\n",
        "\n",
        "-  Reduces risk of wrong classification\n",
        "\n",
        "-  Handles imbalance effectively\n",
        "\n",
        "-  Provides better generalization\n",
        "\n",
        "    **In financial institutions:**\n",
        "\n",
        "\n",
        "-  Reduces risk exposure\n",
        "\n",
        "-  Improves credit decision quality\n",
        "\n",
        "-  Enhances profitability"
      ],
      "metadata": {
        "id": "W1m-C4CEyQ_X"
      }
    }
  ]
}